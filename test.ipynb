{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers\n",
    "!pip -q install sentencepiece\n",
    "!pip -q install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDNN version:  8500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"cuDNN version: \", torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- ----------\n",
      "accelerate                    0.23.0\n",
      "asttokens                     2.4.0\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.5\n",
      "boltons                       23.0.0\n",
      "brotlipy                      0.7.0\n",
      "certifi                       2023.7.22\n",
      "cffi                          1.15.1\n",
      "charset-normalizer            2.0.4\n",
      "cmake                         3.27.6\n",
      "comm                          0.1.4\n",
      "conda                         23.7.4\n",
      "conda-content-trust           0.1.3\n",
      "conda-libmamba-solver         23.5.0\n",
      "conda-package-handling        2.1.0\n",
      "conda_package_streaming       0.8.0\n",
      "cryptography                  39.0.1\n",
      "debugpy                       1.6.7\n",
      "decorator                     5.1.1\n",
      "exceptiongroup                1.1.3\n",
      "executing                     1.2.0\n",
      "filelock                      3.12.4\n",
      "fsspec                        2023.9.2\n",
      "huggingface-hub               0.16.4\n",
      "idna                          3.4\n",
      "importlib-metadata            6.8.0\n",
      "ipykernel                     6.25.2\n",
      "ipython                       8.16.1\n",
      "jedi                          0.19.1\n",
      "Jinja2                        3.1.2\n",
      "jsonpatch                     1.32\n",
      "jsonpointer                   2.1\n",
      "jupyter_client                8.3.1\n",
      "jupyter_core                  5.3.2\n",
      "libmambapy                    1.4.1\n",
      "lit                           17.0.2\n",
      "MarkupSafe                    2.1.3\n",
      "matplotlib-inline             0.1.6\n",
      "mpmath                        1.3.0\n",
      "nest-asyncio                  1.5.6\n",
      "networkx                      3.1\n",
      "numpy                         1.26.0\n",
      "nvidia-cublas-cu11            11.10.3.66\n",
      "nvidia-cuda-cupti-cu11        11.7.101\n",
      "nvidia-cuda-nvrtc-cu11        11.7.99\n",
      "nvidia-cuda-runtime-cu11      11.7.99\n",
      "nvidia-cudnn-cu11             8.5.0.96\n",
      "nvidia-cufft-cu11             10.9.0.58\n",
      "nvidia-curand-cu11            10.2.10.91\n",
      "nvidia-cusolver-cu11          11.4.0.1\n",
      "nvidia-cusparse-cu11          11.7.4.91\n",
      "nvidia-nccl-cu11              2.14.3\n",
      "nvidia-nvtx-cu11              11.7.91\n",
      "packaging                     23.0\n",
      "parso                         0.8.3\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        10.0.1\n",
      "pip                           23.1.2\n",
      "platformdirs                  3.11.0\n",
      "pluggy                        1.0.0\n",
      "prompt-toolkit                3.0.39\n",
      "psutil                        5.9.0\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "pycosat                       0.6.4\n",
      "pycparser                     2.21\n",
      "Pygments                      2.16.1\n",
      "pyOpenSSL                     23.0.0\n",
      "PySocks                       1.7.1\n",
      "python-dateutil               2.8.2\n",
      "PyYAML                        6.0.1\n",
      "pyzmq                         25.1.0\n",
      "regex                         2023.10.3\n",
      "requests                      2.29.0\n",
      "ruamel.yaml                   0.17.21\n",
      "safetensors                   0.3.3\n",
      "sentencepiece                 0.1.99\n",
      "setuptools                    67.8.0\n",
      "six                           1.16.0\n",
      "stack-data                    0.6.2\n",
      "sympy                         1.12\n",
      "tokenizers                    0.14.0\n",
      "toolz                         0.12.0\n",
      "torch                         2.0.1\n",
      "torchaudio                    2.0.2\n",
      "torchvision                   0.15.2\n",
      "tornado                       6.3.2\n",
      "tqdm                          4.65.0\n",
      "traitlets                     5.11.2\n",
      "transformers                  4.34.0\n",
      "triton                        2.0.0\n",
      "typing_extensions             4.8.0\n",
      "urllib3                       1.26.16\n",
      "wcwidth                       0.2.8\n",
      "wheel                         0.38.4\n",
      "zipp                          3.17.0\n",
      "zstandard                     0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichiro/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.87s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# Hugging Face model_path\n",
    "model_path = 'psmathur/orca_mini_3b'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(system, instruction, input=None):\n",
    "\n",
    "    if input:\n",
    "        prompt = f\"### System:\\n{system}\\n\\n### User:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### System:\\n{system}\\n\\n### User:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.LongTensor(tokens).unsqueeze(0)\n",
    "    tokens = tokens.to('cuda')\n",
    "\n",
    "    instance = {'input_ids': tokens,'top_p': 1.0, 'temperature':0.7, 'generate_len': 1024, 'top_k': 50}\n",
    "\n",
    "    length = len(tokens[0])\n",
    "    with torch.no_grad():\n",
    "        rest = model.generate(\n",
    "            input_ids=tokens,\n",
    "            max_length=length+instance['generate_len'],\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "            top_p=instance['top_p'],\n",
    "            temperature=instance['temperature'],\n",
    "            top_k=instance['top_k']\n",
    "        )\n",
    "    output = rest[0][length:]\n",
    "    string = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    return f'[!] Response: {string}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Response: Namami Gange is a government initiative aimed at cleaning the holy river Ganges and its tributaries in India. The initiative involves various stakeholders such as the government, non-governmental organizations, and local communities. As of now, the initiative has been successful in implementing various measures such as building sewage treatment plants, constructing riverfront parks, and promoting water conservation practices to reduce the pollution level of the Ganges river. However, there still remains a long way to go in terms of achieving the ultimate goal of clean and sustainable Ganges river.\n"
     ]
    }
   ],
   "source": [
    "# Sample Test Instruction Used by Youtuber Sam Witteveen https://www.youtube.com/@samwitteveenai\n",
    "system = 'You are AI helper'\n",
    "instruction = 'So is namami gange successful ? '\n",
    "print(generate_text(system, instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ctransformers[cuda]>=0.2.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 10922.67it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q5_K_M.gguf\", model_type=\"mistral\", gpu_layers=150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Namami Ganga Project is a national program launched by the Government of India in 2015 with the aim of cleaning and rejuvenating the River Ganges, also known as the \"Mother River\" of India. The project's primary objectives are:\n",
      "\n",
      "1. To restore the ecological balance of the river and its tributaries by removing pollution and waste.\n",
      "2. To promote sustainable development along the banks of the river through the creation of new job opportunities, economic growth, and improved infrastructure.\n",
      "3. To enhance the cultural and spiritual significance of the River Ganges by promoting cleanliness and respect for its waters.\n",
      "4. To foster a sense of national pride and identity among Indians by revitalizing this important natural resource.\n",
      "5. To promote international cooperation and partnership in conservation efforts related to the river.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"what is namami gange project objective ? answer :\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
